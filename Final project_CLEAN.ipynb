{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3cbcf4",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background: linear-gradient(90deg, #B07CFF, #2D0A47, #000000);\n",
    "    color:white;\n",
    "    padding:25px 30px;\n",
    "    border-radius:12px;\n",
    "    font-size:32px;\n",
    "    font-weight:800;\n",
    "    box-shadow: 0 0 15px rgba(128,0,255,0.6);\n",
    "    width: 95%;        /* Change this to control width */\n",
    "    height: 400;     \n",
    "    line-height: 1.4; \n",
    "\">\n",
    "    <div>DEPI Final Data Science Project</div>\n",
    "    <div>Project: Sales Forecasting and Optimization</div>\n",
    "</div>\n",
    "\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45934592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2dc5d",
   "metadata": {},
   "source": [
    "##### displaying some info "
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b8c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', parse_dates=['Date'], low_memory=False)\n",
    "store = pd.read_csv('data/store.csv')\n",
    "print(f\"Train dataset loaded: {len(train):,} rows, {len(train.columns)} columns\")\n",
    "print(f\"Store dataset loaded: {len(store):,} stores, {len(store.columns)} columns\")\n",
    "print(f\"Date range: {train['Date'].min()} to {train['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eadd61",
   "metadata": {},
   "source": [
    "##### preprocessing step to all add store related attributes in the training data (for max efficiency)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.merge(store, on='Store', how='left')\n",
    "print(f\" Merged dataset: {len(df):,} rows, {len(df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Records: {len(df):,}\")\n",
    "print(f\"Date Range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"Number of Stores: {df['Store'].nunique()}\")\n",
    "print(f\"Time Period: {(df['Date'].max() - df['Date'].min()).days} days\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74cdc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "print(pd.DataFrame({ 'Missing_Count': missing[missing > 0], 'Percentage': missing_pct[missing > 0] }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['Sales', 'Customers', 'CompetitionDistance']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicates: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf5a16",
   "metadata": {},
   "source": [
    "##### outlier detection for \"Sales\" column"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['Sales'].quantile(0.25)\n",
    "Q3 = df['Sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 3 * IQR # we use 3*IQR for stricter outlier detection\n",
    "upper_bound = Q3 + 3 * IQR\n",
    "outliers = df[(df['Sales'] < lower_bound) | (df['Sales'] > upper_bound)]\n",
    "print(f\"Sales outliers: {len(outliers):,} ({len(outliers)/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Sales'] >= lower_bound) & (df['Sales'] <= upper_bound)].copy()\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3dfd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Daily Sales Trend', 'Sales Distribution', 'Sales by Day of Week',\n",
    "        'Promo vs No Promo', 'Sales by Store Type', 'Competition Distance Impact'\n",
    "    ),\n",
    "    specs=[\n",
    "        [{'type': 'scatter'}, {'type': 'histogram'}],\n",
    "        [{'type': 'bar'}, {'type': 'box'}],\n",
    "        [{'type': 'box'}, {'type': 'scatter'}]\n",
    "    ]\n",
    ")\n",
    "daily_sales = df.groupby('Date')['Sales'].mean().reset_index()\n",
    "fig.add_trace(go.Scatter(x=daily_sales['Date'], y=daily_sales['Sales'], mode='markers', name='Avg Daily Sales'), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=df['Sales'], nbinsx=50, name='Sales Dist'), row=1, col=2)\n",
    "dow_sales = df.groupby('DayOfWeek')['Sales'].mean()\n",
    "fig.add_trace(go.Bar(x=['Mon','Tue','Wed','Thu','Fri','Sat','Sun'], y=dow_sales.values, name='DoW Sales'), row=2, col=1)\n",
    "fig.add_trace(go.Box(x=df['Promo'].map({0: 'No Promo', 1: 'Promo'}), y=df['Sales'], name='Promo'), row=2, col=2)\n",
    "if 'StoreType' in df.columns:\n",
    "    fig.add_trace(go.Box(x=df['StoreType'], y=df['Sales'], name='Store Type'), row=3, col=1)\n",
    "if 'CompetitionDistance' in df.columns:\n",
    "    sample = df.sample(min(5000, len(df)))\n",
    "    fig.add_trace(go.Scatter(x=sample['CompetitionDistance'], y=sample['Sales'], mode='markers', marker=dict(size=3, opacity=0.5)), row=3, col=2)\n",
    "fig.update_layout(height=1200,showlegend=False,title_text=\"Rossmann Sales - Initial EDA Dashboard\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e1e5e",
   "metadata": {},
   "source": [
    "##### Data Preprocessing, cleaning and Feature Engineering\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452fe131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop_duplicates()\n",
    "df['CompetitionDistance'].fillna(df['CompetitionDistance'].median(), inplace=True)\n",
    "df['CompetitionOpenSinceMonth'].fillna(0, inplace=True)\n",
    "df['CompetitionOpenSinceYear'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfbce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Promo2SinceWeek'].fillna(0, inplace=True)\n",
    "df['Promo2SinceYear'].fillna(0, inplace=True)\n",
    "df['PromoInterval'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['Sales'].quantile(0.25)\n",
    "Q3 = df['Sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df['Is_Outlier'] = ((df['Sales'] < (Q1 - 3 * IQR)) | (df['Sales'] > (Q3 + 3 * IQR))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88140e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c22d6",
   "metadata": {},
   "source": [
    "##### preparing the a cleaned dataset for feature engineering"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df.copy()\n",
    "df_feat = df_feat.sort_values(['Store', 'Date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e566c",
   "metadata": {},
   "source": [
    "##### extracts calendar components from date column and store them in new columns"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa166a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['Year'] = df_feat['Date'].dt.year\n",
    "df_feat['Month'] = df_feat['Date'].dt.month\n",
    "df_feat['Day'] = df_feat['Date'].dt.day\n",
    "df_feat['WeekOfYear'] = df_feat['Date'].dt.isocalendar().week\n",
    "df_feat['Quarter'] = df_feat['Date'].dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06798d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['IsWeekend'] = (df_feat['DayOfWeek'] >= 6).astype(int) # Saturday=6, Sunday=7, monday=1\n",
    "df_feat['IsMonthStart'] = df_feat['Date'].dt.is_month_start.astype(int)\n",
    "df_feat['IsMonthEnd'] = df_feat['Date'].dt.is_month_end.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c401b",
   "metadata": {},
   "source": [
    "##### this will help machine learning understand cyclical time patterns\n",
    "##### (eg: that december comes january and after saturday comes sunday)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf90ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['Month_sin'] = np.sin(2 * np.pi * df_feat['Month'] / 12)\n",
    "df_feat['Month_cos'] = np.cos(2 * np.pi * df_feat['Month'] / 12)\n",
    "df_feat['DayOfWeek_sin'] = np.sin(2 * np.pi * df_feat['DayOfWeek'] / 7)\n",
    "df_feat['DayOfWeek_cos'] = np.cos(2 * np.pi * df_feat['DayOfWeek'] / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3cfc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = np.arange(1, 13)\n",
    "month_sin = np.sin(2 * np.pi * months / 12)\n",
    "month_cos = np.cos(2 * np.pi * months / 12)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=month_cos,\n",
    "    y=month_sin,\n",
    "    mode='markers+text',\n",
    "    text=[f'Month {m}' for m in months],\n",
    "    textposition='top center',\n",
    "    marker=dict(size=12, color=months, colorscale='Viridis')\n",
    "))\n",
    "# circle outline\n",
    "theta = np.linspace(0, 2 * np.pi, 100)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.cos(theta),\n",
    "    y=np.sin(theta),\n",
    "    mode='lines',\n",
    "    line=dict(color='lightgray', dash='dot'),\n",
    "    showlegend=False\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Cyclical Encoding of Months (sin/cos)',\n",
    "    xaxis_title='cos(2\u03c0 * month / 12)',\n",
    "    yaxis_title='sin(2\u03c0 * month / 12)',\n",
    "    width=600, height=600,\n",
    "    xaxis=dict(scaleanchor='y', scaleratio=1),\n",
    "    yaxis=dict(showgrid=False)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a8ef2",
   "metadata": {},
   "source": [
    "##### represent past values of \u201cSales\u201d and \u201cCustomers\u201d for each store"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb25361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in [1, 7, 14, 30]:#lag periods:1 day, 7 days, 14 days, and 30 days\n",
    "    df_feat[f'Sales_Lag_{lag}'] = df_feat.groupby('Store')['Sales'].shift(lag)\n",
    "    df_feat[f'Customers_Lag_{lag}'] = df_feat.groupby('Store')['Customers'].shift(lag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in [7, 14, 30]:\n",
    "    df_feat[f'Sales_Rolling_Mean_{window}'] = df_feat.groupby('Store')['Sales'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "    df_feat[f'Sales_Rolling_Std_{window}'] = df_feat.groupby('Store')['Sales'].transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "df_feat['SalesPerCustomer'] = df_feat['Sales'] / df_feat['Customers']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ac8da",
   "metadata": {},
   "source": [
    "##### These features help a machine learning model recognize patterns over time and Shows the recent average performance of a store"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat[f'Sales_Rolling_Mean_{window}'] = df_feat.groupby('Store')['Sales'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "df_feat[f'Sales_Rolling_Std_{window}'] = df_feat.groupby('Store')['Sales'].transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "df_feat['SalesPerCustomer'] = df_feat['Sales'] / df_feat['Customers']\n",
    "df_feat['SalesPerCustomer'].replace([np.inf, -np.inf], 0, inplace=True) # if record has no customers, set SalesPerCustomer to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcee14d",
   "metadata": {},
   "source": [
    "##### this block creates a new feature called CompetitionMonthsOpen, which measures how many months a store has had competition nearby up to the current date (based on the record\u2019s year and month)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d24c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'CompetitionOpenSinceYear' in df_feat.columns:\n",
    "        #calculates how many months have passed since a store\u2019s competition started\n",
    "    df_feat['CompetitionMonthsOpen'] = 12 * (df_feat['Year'] - df_feat['CompetitionOpenSinceYear']) + (df_feat['Month'] - df_feat['CompetitionOpenSinceMonth'])\n",
    "    df_feat['CompetitionMonthsOpen'] = df_feat['CompetitionMonthsOpen'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffff4e2",
   "metadata": {},
   "source": [
    "##### it measures how long a store\u2019s continuous promotion program (\u201cPromo2\u201d) has been running"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Promo2SinceYear' in df_feat.columns:\n",
    "    df_feat['Promo2Weeks'] = 52 * (df_feat['Year'] - df_feat['Promo2SinceYear']) + (df_feat['WeekOfYear'] - df_feat['Promo2SinceWeek'])\n",
    "    df_feat['Promo2Weeks'] = df_feat['Promo2Weeks'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb72e2",
   "metadata": {},
   "source": [
    "##### finally ensuring that the final dataset has no missing values before passing it into a machine learning model"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df_feat.dropna()  \n",
    "print(f\"Feature engineered shape: {df_feat.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462dc7e",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d3e553e",
   "metadata": {},
   "source": [
    "##### The Augmented Dickey-Fuller test for stationarity on the daily aggregated Rossmann sales data yields an ADF statistic of -70.56 and a p-value below 0.0001. We conclusively reject the null hypothesis of a unit root, indicating that the time series is stationary and suitable for time series modeling without further differencing.\"\n",
    "- we\u2019re ready to proceed to training time series models (AR, ARIMA, SARIMAX, Prophet, etc.)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ea809",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df_feat['Sales'].sample(50000, random_state=1)\n",
    "adf_result = adfuller(subset.dropna())\n",
    "print(f\"ADF Statistic: {adf_result[0]:.4f}\")\n",
    "print(f\"p-value: {adf_result[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df_feat.select_dtypes(include=[np.number]).columns.tolist()\n",
    "correlation_matrix = df_feat[numeric_cols].corr()\n",
    "sales_corr = correlation_matrix['Sales'].abs().sort_values(ascending=False)\n",
    "print(\"Top 15 features correlated with Sales:\")\n",
    "print(sales_corr.head(48)[1:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee812fd8",
   "metadata": {},
   "source": [
    "##### measures how much promotions increase sales on average"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_impact = df_feat.groupby('Promo')['Sales'].agg(['mean', 'median', 'std'])\n",
    "promo_impact.index = ['No Promo', 'With Promo']\n",
    "promo_lift = ((promo_impact.loc['With Promo', 'mean'] / promo_impact.loc['No Promo', 'mean']) - 1) * 100\n",
    "print(f\"Avg sales lift from promotions: {promo_lift:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'StoreType' in df_feat.columns:\n",
    "    print(df_feat.groupby('StoreType')['Sales'].agg(['mean', 'median', 'std', 'count']))\n",
    "dow_stats = df_feat.groupby('DayOfWeek')['Sales'].agg(['mean', 'median'])\n",
    "dow_stats.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "print(dow_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "top_features = correlation_matrix['Sales'].abs().sort_values(ascending=False).head(20).index\n",
    "sns.heatmap(correlation_matrix.loc[top_features, top_features], annot=True, cmap='coolwarm', center=0, fmt='.2f', square=True)\n",
    "plt.title('Top 20 Features - Correlation Heatmap', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sales = df_feat.groupby('Date')['Sales'].mean().dropna().sample(min(900, len(df_feat)))\n",
    "# Compute ACF and PACF values (up to 50 lags)\n",
    "lags = 50\n",
    "acf_values = acf(sample_sales, nlags=lags)\n",
    "pacf_values = pacf(sample_sales, nlags=lags)\n",
    "# Create a 1x2 subplot layout\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Autocorrelation Function (ACF)\", \"Partial Autocorrelation (PACF)\"))\n",
    "# ACF plot\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(range(lags + 1)), y=acf_values, name='ACF', marker_color='skyblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "# PACF plot\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(range(lags + 1)), y=pacf_values, name='PACF', marker_color='lightgreen'),\n",
    "    row=1, col=2\n",
    ")\n",
    "# Add horizontal zero lines\n",
    "for i in range(1, 3):\n",
    "    fig.add_shape(type=\"line\", x0=0, x1=lags, y0=0, y1=0, line=dict(color=\"black\", width=1), row=1, col=i)\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"ACF and PACF Plots (Interactive)\",\n",
    "    showlegend=False,\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.to_csv('data/cleaned_sales_features.csv', index=False)\n",
    "print(\"\u2705 Cleaned feature dataset saved to: data/cleaned_sales_features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8f19e",
   "metadata": {},
   "source": [
    "## Forecasting Model Development and Optimization Objectives"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "#how to fix pmdarima installation issues?\n",
    "#import pmdarima as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5292736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cleaned_sales_features.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9971bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sales = df.groupby('Date').agg({\n",
    "    'Sales': 'sum',\n",
    "    'Customers': 'sum', \n",
    "    'Promo': 'mean',\n",
    "    'SchoolHoliday': 'max',\n",
    "    'StateHoliday': lambda x: (x != '0').any().astype(int)\n",
    "}).reset_index()\n",
    "daily_sales = daily_sales.sort_values('Date')\n",
    "print(f\"Time Series Data: {len(daily_sales)} days\")\n",
    "print(f\"Date Range: {daily_sales['Date'].min()} to {daily_sales['Date'].max()}\")\n",
    "print(f\"Total Sales Range: {daily_sales['Sales'].min():,.0f} to {daily_sales['Sales'].max():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e6ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=daily_sales['Date'], y=daily_sales['Sales'], \n",
    "                        mode='lines', name='Daily Sales',\n",
    "                        line=dict(color='blue', width=1)))\n",
    "fig.update_layout(\n",
    "    title='Rossmann Daily Sales Time Series',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Total Daily Sales',\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4cf75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weeks = 23\n",
    "test_size = test_weeks * 7\n",
    "train_size = len(daily_sales) - test_size\n",
    "\n",
    "train_data = daily_sales.iloc[:train_size].copy()\n",
    "test_data = daily_sales.iloc[train_size:].copy()\n",
    "\n",
    "print(f\"Training Data: {len(train_data)} days ({train_data['Date'].min()} to {train_data['Date'].max()})\")\n",
    "print(f\"Test Data: {len(test_data)} days ({test_data['Date'].min()} to {test_data['Date'].max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcedb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=train_data['Date'], y=train_data['Sales'],\n",
    "                        mode='lines', name='Training Data', line=dict(color='blue')))\n",
    "fig.add_trace(go.Scatter(x=test_data['Date'], y=test_data['Sales'],\n",
    "                        mode='lines', name='Test Data', line=dict(color='red')))\n",
    "fig.update_layout(title='Train-Test Split Visualization', height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_auto_arima(train_series, seasonal=True):\n",
    "    # Define potential parameter ranges (p,d,q)\n",
    "    p_values = range(0, 3)\n",
    "    d_values = range(0, 2)\n",
    "    q_values = range(0, 3)\n",
    "    best_aic = np.inf\n",
    "    best_order = None\n",
    "    best_model = None\n",
    "\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p, d, q)\n",
    "                try:\n",
    "                    model = ARIMA(train_series, order=order)\n",
    "                    results = model.fit()\n",
    "                    if results.aic < best_aic:\n",
    "                        best_aic = results.aic\n",
    "                        best_order = order\n",
    "                        best_model = results\n",
    "                    print(f\"Tested order {order}, AIC: {results.aic:.2f}\")\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    print(f\"\u2705 Selected ARIMA order: {best_order} (AIC={best_aic:.2f})\")\n",
    "    return best_model\n",
    "# Train \"Auto-ARIMA\"\n",
    "arima_model = train_auto_arima(train_data['Sales'])\n",
    "# Make predictions\n",
    "arima_forecast = arima_model.forecast(steps=len(test_data))\n",
    "# (statsmodels forecast doesn't return conf int by default, but you can add if needed)\n",
    "print(\"\ud83c\udfaf ARIMA predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prophet_model(train_data):    \n",
    "    # Prepare data for Prophet\n",
    "    prophet_train = train_data[['Date', 'Sales']].copy()\n",
    "    prophet_train.columns = ['ds', 'y']\n",
    "    \n",
    "    # Add external regressors\n",
    "    prophet_train['promo'] = train_data['Promo'].values\n",
    "    prophet_train['school_holiday'] = train_data['SchoolHoliday'].values\n",
    "    \n",
    "    # Initialize Prophet model\n",
    "    prophet_model = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.05,\n",
    "        seasonality_prior_scale=10.0\n",
    "    )\n",
    "    # Add external regressors\n",
    "    prophet_model.add_regressor('promo')\n",
    "    prophet_model.add_regressor('school_holiday')\n",
    "    # Fit model\n",
    "    prophet_model.fit(prophet_train)\n",
    "    print(\"\u2705 Prophet model trained successfully!\")\n",
    "    return prophet_model\n",
    "\n",
    "# Train Prophet\n",
    "prophet_model = train_prophet_model(train_data)\n",
    "# Make predictions\n",
    "future_prophet = prophet_model.make_future_dataframe(periods=len(test_data))\n",
    "future_prophet['promo'] = pd.concat([train_data['Promo'], test_data['Promo']], ignore_index=True)\n",
    "future_prophet['school_holiday'] = pd.concat([train_data['SchoolHoliday'], test_data['SchoolHoliday']], ignore_index=True)\n",
    "prophet_forecast = prophet_model.predict(future_prophet)\n",
    "prophet_predictions = prophet_forecast.iloc[-len(test_data):]['yhat'].values\n",
    "\n",
    "print(\"\ud83c\udfaf Prophet predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d37252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df):    \n",
    "    df_features = df.copy()\n",
    "    df_features['year'] = df_features['Date'].dt.year\n",
    "    df_features['month'] = df_features['Date'].dt.month\n",
    "    df_features['day'] = df_features['Date'].dt.day\n",
    "    df_features['dayofweek'] = df_features['Date'].dt.dayofweek\n",
    "    df_features['dayofyear'] = df_features['Date'].dt.dayofyear\n",
    "    df_features['weekofyear'] = df_features['Date'].dt.isocalendar().week\n",
    "    df_features['quarter'] = df_features['Date'].dt.quarter\n",
    "    df_features['is_weekend'] = (df_features['dayofweek'] >= 5).astype(int)\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['day_sin'] = np.sin(2 * np.pi * df_features['dayofweek'] / 7)\n",
    "    df_features['day_cos'] = np.cos(2 * np.pi * df_features['dayofweek'] / 7)\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 7, 14, 21]:\n",
    "        df_features[f'sales_lag_{lag}'] = df_features['Sales'].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for window in [7, 14, 28]:\n",
    "        df_features[f'sales_rolling_mean_{window}'] = df_features['Sales'].rolling(window).mean()\n",
    "        df_features[f'sales_rolling_std_{window}'] = df_features['Sales'].rolling(window).std()\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "def train_xgboost_model(train_data, test_data):\n",
    "    \"\"\"Train XGBoost model with Bayesian optimization\"\"\"\n",
    "    \n",
    "    print(\"\ud83d\ude80 Training XGBoost Model with Bayesian Optimization...\")\n",
    "    \n",
    "    # Create features\n",
    "    train_features = create_time_features(train_data)\n",
    "    test_features = create_time_features(test_data)\n",
    "    \n",
    "    # Select feature columns\n",
    "    feature_cols = [col for col in train_features.columns if col not in ['Date', 'Sales']]\n",
    "    \n",
    "    # Drop NaN rows (from lag features)\n",
    "    train_clean = train_features.dropna()\n",
    "    \n",
    "    X_train = train_clean[feature_cols]\n",
    "    y_train = train_clean['Sales']\n",
    "    X_test = test_features[feature_cols].fillna(method='ffill')\n",
    "    \n",
    "    # Bayesian optimization for hyperparameters\n",
    "    search_spaces = {\n",
    "        'n_estimators': Integer(100, 1000),\n",
    "        'max_depth': Integer(3, 10),\n",
    "        'learning_rate': Real(0.01, 0.3),\n",
    "        'subsample': Real(0.6, 1.0),\n",
    "        'colsample_bytree': Real(0.6, 1.0)\n",
    "    }\n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "    \n",
    "    # Time series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    bayes_search = BayesSearchCV(\n",
    "        xgb_model,\n",
    "        search_spaces,\n",
    "        n_iter=20,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )    \n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    print(f\"\u2705 Best XGBoost Parameters: {bayes_search.best_params_}\")\n",
    "    # Make predictions\n",
    "    xgb_predictions = bayes_search.predict(X_test)\n",
    "    \n",
    "    return bayes_search.best_estimator_, xgb_predictions\n",
    "# Train XGBoost\n",
    "xgb_model, xgb_predictions = train_xgboost_model(train_data, test_data)\n",
    "\n",
    "print(\"\ud83c\udfaf XGBoost predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_sequences(data, lookback=14):\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(data)):\n",
    "        X.append(data[i-lookback:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def train_lstm_model(train_data, test_data, lookback=14):    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_data[['Sales']])\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train, y_train = create_lstm_sequences(train_scaled.flatten(), lookback)\n",
    "    \n",
    "    # Reshape for LSTM [samples, time steps, features]\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    \n",
    "    # Build LSTM model\n",
    "    lstm_model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(lookback, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    \n",
    "    # Train model\n",
    "    history = lstm_model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    last_sequence = train_scaled[-lookback:].flatten()\n",
    "    lstm_predictions = []\n",
    "    \n",
    "    for _ in range(len(test_data)):\n",
    "        # Predict next value\n",
    "        pred_input = last_sequence[-lookback:].reshape(1, lookback, 1)\n",
    "        pred = lstm_model.predict(pred_input, verbose=0)[0, 0]\n",
    "        lstm_predictions.append(pred)\n",
    "        \n",
    "        # Update sequence\n",
    "        last_sequence = np.append(last_sequence[1:], pred)\n",
    "    \n",
    "    # Inverse scale predictions\n",
    "    lstm_predictions = scaler.inverse_transform(np.array(lstm_predictions).reshape(-1, 1)).flatten()\n",
    "    \n",
    "    print(\"\u2705 LSTM model trained successfully!\")\n",
    "    return lstm_model, lstm_predictions, scaler\n",
    "\n",
    "# Train LSTM\n",
    "lstm_model, lstm_predictions, lstm_scaler = train_lstm_model(train_data, test_data)\n",
    "\n",
    "print(\"\ud83c\udfaf LSTM predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(train_data, test_data):    \n",
    "    # Create features (reuse from XGBoost)\n",
    "    train_features = create_time_features(train_data)\n",
    "    test_features = create_time_features(test_data)\n",
    "    \n",
    "    feature_cols = [col for col in train_features.columns if col not in ['Date', 'Sales']]\n",
    "    \n",
    "    train_clean = train_features.dropna()\n",
    "    X_train = train_clean[feature_cols]\n",
    "    y_train = train_clean['Sales']\n",
    "    X_test = test_features[feature_cols].fillna(method='ffill')\n",
    "    \n",
    "    # Hyperparameter search\n",
    "    search_spaces = {\n",
    "        'n_estimators': Integer(100, 500),\n",
    "        'max_depth': Integer(5, 20),\n",
    "        'min_samples_split': Integer(2, 10),\n",
    "        'min_samples_leaf': Integer(1, 5)\n",
    "    }\n",
    "    \n",
    "    rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    bayes_search = BayesSearchCV(\n",
    "        rf_model,\n",
    "        search_spaces,\n",
    "        n_iter=15,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    \n",
    "    rf_predictions = bayes_search.predict(X_test)\n",
    "    \n",
    "    print(f\"\u2705 Best Random Forest Parameters: {bayes_search.best_params_}\")\n",
    "    return bayes_search.best_estimator_, rf_predictions\n",
    "# Train Random Forest\n",
    "rf_model, rf_predictions = train_random_forest(train_data, test_data)\n",
    "\n",
    "print(\"\ud83c\udfaf Random Forest predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e6717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "# Collect all predictions\n",
    "predictions = {\n",
    "    'Auto-ARIMA': arima_forecast,\n",
    "    'Facebook Prophet': prophet_predictions,\n",
    "    'XGBoost': xgb_predictions,\n",
    "    'LSTM': lstm_predictions,\n",
    "    'Random Forest': rf_predictions\n",
    "}\n",
    "\n",
    "# Calculate metrics for all models\n",
    "results = []\n",
    "y_true = test_data['Sales'].values\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    metrics = calculate_metrics(y_true, y_pred, model_name)\n",
    "    results.append(metrics)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('RMSE')\n",
    "\n",
    "print(\"\ud83d\udcca MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df.to_string(index=False, float_format='%.2f'))\n",
    "\n",
    "# Visualize model comparison\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "                   subplot_titles=['RMSE Comparison', 'MAE Comparison', \n",
    "                                 'MAPE Comparison', 'Actual vs Predictions'])\n",
    "results_df['RMSE'] = results_df['RMSE'].round(2)\n",
    "results_df['MAE'] = results_df['MAE'].round(2)\n",
    "results_df['MAPE'] = results_df['MAPE'].round(2)\n",
    "# RMSE\n",
    "fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['RMSE'], name='RMSE'), row=1, col=1)\n",
    "\n",
    "# MAE\n",
    "fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['MAE'], name='MAE'), row=1, col=2)\n",
    "\n",
    "# MAPE\n",
    "fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['MAPE'], name='MAPE'), row=2, col=1)\n",
    "\n",
    "# Actual vs Predictions (Best Model)\n",
    "best_model = results_df.iloc[0]['Model']\n",
    "best_predictions = predictions[best_model]\n",
    "\n",
    "fig.add_trace(go.Scatter(x=test_data['Date'], y=y_true, mode='lines', \n",
    "                        name='Actual', line=dict(color='blue')), row=2, col=2)\n",
    "fig.add_trace(go.Scatter(x=test_data['Date'], y=best_predictions, mode='lines',\n",
    "                        name=f'{best_model} (Best)', line=dict(color='red')), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Comprehensive Model Performance Analysis\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27797ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
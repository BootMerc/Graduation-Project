{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3cbcf4",
   "metadata": {},
   "source": [
    "# DEPI Final Data Science Project \n",
    "## Project: Sales Forecasting and Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45934592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c274fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Create visualization directory\n",
    "viz_dir = Path('visualizations')\n",
    "viz_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def save_plotly_chart(fig, filename):\n",
    "    \"\"\"Save Plotly chart externally (not embedded)\"\"\"\n",
    "    fig.write_html(viz_dir / filename)\n",
    "    print(f\"âœ… Chart saved: {filename}\")\n",
    "\n",
    "def save_plt_chart(title, dpi=100):\n",
    "    \"\"\"Save matplotlib chart externally\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.savefig(viz_dir / f'{title}.png', dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Figure saved: {title}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223e16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_optimization import NotebookOptimizer\n",
    "\n",
    "optimizer = NotebookOptimizer(\"Final project.ipynb\")\n",
    "#optimizer.strip_outputs()\n",
    "#optimizer.clean_metadata()\n",
    "#optimizer.save_clean_version()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2dc5d",
   "metadata": {},
   "source": [
    "##### displaying some info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b8c9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset loaded: 1,017,209 rows, 9 columns\n",
      "Store dataset loaded: 1,115 stores, 10 columns\n",
      "Date range: 2013-01-01 00:00:00 to 2015-07-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv', parse_dates=['Date'], low_memory=False)\n",
    "store = pd.read_csv('data/store.csv')\n",
    "print(f\"Train dataset loaded: {len(train):,} rows, {len(train.columns)} columns\")\n",
    "print(f\"Store dataset loaded: {len(store):,} stores, {len(store.columns)} columns\")\n",
    "print(f\"Date range: {train['Date'].min()} to {train['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eadd61",
   "metadata": {},
   "source": [
    "##### preprocessing step to all add store related attributes in the training data (for max efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4984c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Merged dataset: 1,017,209 rows, 18 columns\n"
     ]
    }
   ],
   "source": [
    "df = train.merge(store, on='Store', how='left')\n",
    "print(f\" Merged dataset: {len(df):,} rows, {len(df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c10f228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records: 1,017,209\n",
      "Date Range: 2013-01-01 00:00:00 to 2015-07-31 00:00:00\n",
      "Number of Stores: 1115\n",
      "Time Period: 941 days\n",
      "Store                                 int64\n",
      "DayOfWeek                             int64\n",
      "Date                         datetime64[ns]\n",
      "Sales                                 int64\n",
      "Customers                             int64\n",
      "Open                                  int64\n",
      "Promo                                 int64\n",
      "StateHoliday                         object\n",
      "SchoolHoliday                         int64\n",
      "StoreType                            object\n",
      "Assortment                           object\n",
      "CompetitionDistance                 float64\n",
      "CompetitionOpenSinceMonth           float64\n",
      "CompetitionOpenSinceYear            float64\n",
      "Promo2                                int64\n",
      "Promo2SinceWeek                     float64\n",
      "Promo2SinceYear                     float64\n",
      "PromoInterval                        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Records: {len(df):,}\")\n",
    "print(f\"Date Range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"Number of Stores: {df['Store'].nunique()}\")\n",
    "print(f\"Time Period: {(df['Date'].max() - df['Date'].min()).days} days\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74cdc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Missing_Count  Percentage\n",
      "CompetitionDistance                 2642    0.259730\n",
      "CompetitionOpenSinceMonth         323348   31.787764\n",
      "CompetitionOpenSinceYear          323348   31.787764\n",
      "Promo2SinceWeek                   508031   49.943620\n",
      "Promo2SinceYear                   508031   49.943620\n",
      "PromoInterval                     508031   49.943620\n"
     ]
    }
   ],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "print(pd.DataFrame({ 'Missing_Count': missing[missing > 0], 'Percentage': missing_pct[missing > 0] }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329d223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Sales     Customers  CompetitionDistance\n",
      "count  1.017209e+06  1.017209e+06         1.014567e+06\n",
      "mean   5.773819e+03  6.331459e+02         5.430086e+03\n",
      "std    3.849926e+03  4.644117e+02         7.715324e+03\n",
      "min    0.000000e+00  0.000000e+00         2.000000e+01\n",
      "25%    3.727000e+03  4.050000e+02         7.100000e+02\n",
      "50%    5.744000e+03  6.090000e+02         2.330000e+03\n",
      "75%    7.856000e+03  8.370000e+02         6.890000e+03\n",
      "max    4.155100e+04  7.388000e+03         7.586000e+04\n"
     ]
    }
   ],
   "source": [
    "print(df[['Sales', 'Customers', 'CompetitionDistance']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a4e537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Duplicates: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf5a16",
   "metadata": {},
   "source": [
    "##### outlier detection for \"Sales\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad6426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales outliers: 3,772 (0.37%)\n"
     ]
    }
   ],
   "source": [
    "Q1 = df['Sales'].quantile(0.25)\n",
    "Q3 = df['Sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 3 * IQR # we use 3*IQR for stricter outlier detection\n",
    "upper_bound = Q3 + 3 * IQR\n",
    "outliers = df[(df['Sales'] < lower_bound) | (df['Sales'] > upper_bound)]\n",
    "print(f\"Sales outliers: {len(outliers):,} ({len(outliers)/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63f7ba82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1017209, 18)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c534dac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset shape: (1013437, 18)\n"
     ]
    }
   ],
   "source": [
    "df = df[(df['Sales'] >= lower_bound) & (df['Sales'] <= upper_bound)].copy()\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af3dfd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chart saved: Rossmann Sales - Initial EDA Dashboard.html\n"
     ]
    }
   ],
   "source": [
    "fig = make_subplots(rows=3, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Daily Sales Trend', 'Sales Distribution', 'Sales by Day of Week',\n",
    "        'Promo vs No Promo', 'Sales by Store Type', 'Competition Distance Impact'\n",
    "    ),\n",
    "    specs=[\n",
    "        [{'type': 'scatter'}, {'type': 'histogram'}],\n",
    "        [{'type': 'bar'}, {'type': 'box'}],\n",
    "        [{'type': 'box'}, {'type': 'scatter'}]\n",
    "    ]\n",
    ")\n",
    "daily_sales = df.groupby('Date')['Sales'].mean().reset_index()\n",
    "fig.add_trace(go.Scatter(x=daily_sales['Date'], y=daily_sales['Sales'], mode='markers', name='Avg Daily Sales'), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=df['Sales'], nbinsx=50, name='Sales Dist'), row=1, col=2)\n",
    "dow_sales = df.groupby('DayOfWeek')['Sales'].mean()\n",
    "fig.add_trace(go.Bar(x=['Mon','Tue','Wed','Thu','Fri','Sat','Sun'], y=dow_sales.values, name='DoW Sales'), row=2, col=1)\n",
    "fig.add_trace(go.Box(x=df['Promo'].map({0: 'No Promo', 1: 'Promo'}), y=df['Sales'], name='Promo'), row=2, col=2)\n",
    "if 'StoreType' in df.columns:\n",
    "    fig.add_trace(go.Box(x=df['StoreType'], y=df['Sales'], name='Store Type'), row=3, col=1)\n",
    "if 'CompetitionDistance' in df.columns:\n",
    "    sample = df.sample(min(5000, len(df)))\n",
    "    fig.add_trace(go.Scatter(x=sample['CompetitionDistance'], y=sample['Sales'], mode='markers', marker=dict(size=3, opacity=0.5)), row=3, col=2)\n",
    "fig.update_layout(height=1200,showlegend=False,title_text=\"Rossmann Sales - Initial EDA Dashboard\")\n",
    "save_plotly_chart(fig, 'Rossmann Sales - Initial EDA Dashboard.html')  # âœ… Saves externally!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e1e5e",
   "metadata": {},
   "source": [
    "##### Data Preprocessing, cleaning and Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "452fe131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop_duplicates()\n",
    "df['CompetitionDistance'].fillna(df['CompetitionDistance'].median(), inplace=True)\n",
    "df['CompetitionOpenSinceMonth'].fillna(0, inplace=True)\n",
    "df['CompetitionOpenSinceYear'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bfbce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Promo2SinceWeek'].fillna(0, inplace=True)\n",
    "df['Promo2SinceYear'].fillna(0, inplace=True)\n",
    "df['PromoInterval'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed53f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['Sales'].quantile(0.25)\n",
    "Q3 = df['Sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df['Is_Outlier'] = ((df['Sales'] < (Q1 - 3 * IQR)) | (df['Sales'] > (Q3 + 3 * IQR))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88140e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store                        0\n",
       "DayOfWeek                    0\n",
       "Date                         0\n",
       "Sales                        0\n",
       "Customers                    0\n",
       "Open                         0\n",
       "Promo                        0\n",
       "StateHoliday                 0\n",
       "SchoolHoliday                0\n",
       "StoreType                    0\n",
       "Assortment                   0\n",
       "CompetitionDistance          0\n",
       "CompetitionOpenSinceMonth    0\n",
       "CompetitionOpenSinceYear     0\n",
       "Promo2                       0\n",
       "Promo2SinceWeek              0\n",
       "Promo2SinceYear              0\n",
       "PromoInterval                0\n",
       "Is_Outlier                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c22d6",
   "metadata": {},
   "source": [
    "##### preparing the a cleaned dataset for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "986cc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df.copy()\n",
    "df_feat = df_feat.sort_values(['Store', 'Date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e566c",
   "metadata": {},
   "source": [
    "##### extracts calendar components from date column and store them in new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa166a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['Year'] = df_feat['Date'].dt.year\n",
    "df_feat['Month'] = df_feat['Date'].dt.month\n",
    "df_feat['Day'] = df_feat['Date'].dt.day\n",
    "df_feat['WeekOfYear'] = df_feat['Date'].dt.isocalendar().week\n",
    "df_feat['Quarter'] = df_feat['Date'].dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06798d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['IsWeekend'] = (df_feat['DayOfWeek'] >= 6).astype(int) # Saturday=6, Sunday=7, monday=1\n",
    "df_feat['IsMonthStart'] = df_feat['Date'].dt.is_month_start.astype(int)\n",
    "df_feat['IsMonthEnd'] = df_feat['Date'].dt.is_month_end.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c401b",
   "metadata": {},
   "source": [
    "##### this will help machine learning understand cyclical time patterns\n",
    "##### (eg: that december comes january and after saturday comes sunday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bf90ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['Month_sin'] = np.sin(2 * np.pi * df_feat['Month'] / 12)\n",
    "df_feat['Month_cos'] = np.cos(2 * np.pi * df_feat['Month'] / 12)\n",
    "df_feat['DayOfWeek_sin'] = np.sin(2 * np.pi * df_feat['DayOfWeek'] / 7)\n",
    "df_feat['DayOfWeek_cos'] = np.cos(2 * np.pi * df_feat['DayOfWeek'] / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca3cfc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chart saved: Cyclical Encoding of Months (sin_cos).html\n"
     ]
    }
   ],
   "source": [
    "months = np.arange(1, 13)\n",
    "month_sin = np.sin(2 * np.pi * months / 12)\n",
    "month_cos = np.cos(2 * np.pi * months / 12)\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=month_cos,\n",
    "    y=month_sin,\n",
    "    mode='markers+text',\n",
    "    text=[f'Month {m}' for m in months],\n",
    "    textposition='top center',\n",
    "    marker=dict(size=12, color=months, colorscale='Viridis')\n",
    "))\n",
    "# circle outline\n",
    "theta = np.linspace(0, 2 * np.pi, 100)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.cos(theta),\n",
    "    y=np.sin(theta),\n",
    "    mode='lines',\n",
    "    line=dict(color='lightgray', dash='dot'),\n",
    "    showlegend=False\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Cyclical Encoding of Months (sin/cos)',\n",
    "    xaxis_title='cos(2Ï€ * month / 12)',\n",
    "    yaxis_title='sin(2Ï€ * month / 12)',\n",
    "    width=600, height=600,\n",
    "    xaxis=dict(scaleanchor='y', scaleratio=1),\n",
    "    yaxis=dict(showgrid=False)\n",
    ")\n",
    "save_plotly_chart(fig, 'Cyclical Encoding of Months (sin_cos).html')  # âœ… Saves externally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a8ef2",
   "metadata": {},
   "source": [
    "##### represent past values of â€œSalesâ€ and â€œCustomersâ€ for each store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb25361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in [1, 7, 14, 30]:#lag periods:1 day, 7 days, 14 days, and 30 days\n",
    "    df_feat[f'Sales_Lag_{lag}'] = df_feat.groupby('Store')['Sales'].shift(lag)\n",
    "    df_feat[f'Customers_Lag_{lag}'] = df_feat.groupby('Store')['Customers'].shift(lag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73dd0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in [7, 14, 30]:\n",
    "    df_feat[f'Sales_Rolling_Mean_{window}'] = df_feat.groupby('Store')['Sales'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "    df_feat[f'Sales_Rolling_Std_{window}'] = df_feat.groupby('Store')['Sales'].transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "df_feat['SalesPerCustomer'] = df_feat['Sales'] / df_feat['Customers']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ac8da",
   "metadata": {},
   "source": [
    "##### These features help a machine learning model recognize patterns over time and Shows the recent average performance of a store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6455eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat[f'Sales_Rolling_Mean_{window}'] = df_feat.groupby('Store')['Sales'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "df_feat[f'Sales_Rolling_Std_{window}'] = df_feat.groupby('Store')['Sales'].transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "df_feat['SalesPerCustomer'] = df_feat['Sales'] / df_feat['Customers']\n",
    "df_feat['SalesPerCustomer'].replace([np.inf, -np.inf], 0, inplace=True) # if record has no customers, set SalesPerCustomer to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcee14d",
   "metadata": {},
   "source": [
    "##### this block creates a new feature called CompetitionMonthsOpen, which measures how many months a store has had competition nearby up to the current date (based on the recordâ€™s year and month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d24c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'CompetitionOpenSinceYear' in df_feat.columns:\n",
    "        #calculates how many months have passed since a storeâ€™s competition started\n",
    "    df_feat['CompetitionMonthsOpen'] = 12 * (df_feat['Year'] - df_feat['CompetitionOpenSinceYear']) + (df_feat['Month'] - df_feat['CompetitionOpenSinceMonth'])\n",
    "    df_feat['CompetitionMonthsOpen'] = df_feat['CompetitionMonthsOpen'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffff4e2",
   "metadata": {},
   "source": [
    "##### it measures how long a storeâ€™s continuous promotion program (â€œPromo2â€) has been running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fefd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Promo2SinceYear' in df_feat.columns:\n",
    "    df_feat['Promo2Weeks'] = 52 * (df_feat['Year'] - df_feat['Promo2SinceYear']) + (df_feat['WeekOfYear'] - df_feat['Promo2SinceWeek'])\n",
    "    df_feat['Promo2Weeks'] = df_feat['Promo2Weeks'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb72e2",
   "metadata": {},
   "source": [
    "##### finally ensuring that the final dataset has no missing values before passing it into a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4abc9899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineered shape: (812824, 48)\n"
     ]
    }
   ],
   "source": [
    "df_feat = df_feat.dropna()  \n",
    "print(f\"Feature engineered shape: {df_feat.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462dc7e",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e553e",
   "metadata": {},
   "source": [
    "##### The Augmented Dickey-Fuller test for stationarity on the daily aggregated Rossmann sales data yields an ADF statistic of -70.56 and a p-value below 0.0001. We conclusively reject the null hypothesis of a unit root, indicating that the time series is stationary and suitable for time series modeling without further differencing.\"\n",
    "- weâ€™re ready to proceed to training time series models (AR, ARIMA, SARIMAX, Prophet, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa1ea809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF Statistic: -129.7046\n",
      "p-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "subset = df_feat['Sales'].sample(50000, random_state=1)\n",
    "adf_result = adfuller(subset.dropna())\n",
    "print(f\"ADF Statistic: {adf_result[0]:.4f}\")\n",
    "print(f\"p-value: {adf_result[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4ea7949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 features correlated with Sales:\n",
      "Customers                    0.804878\n",
      "Sales_Rolling_Mean_7         0.796451\n",
      "Sales_Rolling_Mean_30        0.777618\n",
      "Sales_Rolling_Mean_14        0.772597\n",
      "Sales_Rolling_Std_7          0.733644\n",
      "Sales_Lag_14                 0.701175\n",
      "Sales_Rolling_Std_30         0.684401\n",
      "Sales_Rolling_Std_14         0.675307\n",
      "Customers_Lag_14             0.614343\n",
      "Customers_Lag_7              0.520310\n",
      "Sales_Lag_7                  0.502012\n",
      "Promo                        0.380655\n",
      "Sales_Lag_30                 0.373012\n",
      "Customers_Lag_30             0.371565\n",
      "Sales_Lag_1                  0.350079\n",
      "Customers_Lag_1              0.315951\n",
      "SalesPerCustomer             0.205977\n",
      "DayOfWeek                    0.184199\n",
      "IsWeekend                    0.161697\n",
      "DayOfWeek_sin                0.130156\n",
      "Promo2Weeks                  0.119075\n",
      "Promo2SinceYear              0.119041\n",
      "Promo2                       0.119016\n",
      "IsMonthEnd                   0.064703\n",
      "WeekOfYear                   0.063337\n",
      "Month                        0.061923\n",
      "Is_Outlier                   0.055507\n",
      "IsMonthStart                 0.053217\n",
      "Day                          0.052378\n",
      "Quarter                      0.052177\n",
      "Promo2SinceWeek              0.050696\n",
      "SchoolHoliday                0.040030\n",
      "Year                         0.032975\n",
      "DayOfWeek_cos                0.027979\n",
      "CompetitionDistance          0.025811\n",
      "Month_cos                    0.025580\n",
      "CompetitionOpenSinceMonth    0.014920\n",
      "Month_sin                    0.014487\n",
      "CompetitionOpenSinceYear     0.005819\n",
      "CompetitionMonthsOpen        0.005765\n",
      "Store                        0.003399\n",
      "Open                              NaN\n",
      "Name: Sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = df_feat.select_dtypes(include=[np.number]).columns.tolist()\n",
    "correlation_matrix = df_feat[numeric_cols].corr()\n",
    "sales_corr = correlation_matrix['Sales'].abs().sort_values(ascending=False)\n",
    "print(\"Top 15 features correlated with Sales:\")\n",
    "print(sales_corr.head(48)[1:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee812fd8",
   "metadata": {},
   "source": [
    "##### measures how much promotions increase sales on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7401acfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg sales lift from promotions: 37.70%\n"
     ]
    }
   ],
   "source": [
    "promo_impact = df_feat.groupby('Promo')['Sales'].agg(['mean', 'median', 'std'])\n",
    "promo_impact.index = ['No Promo', 'With Promo']\n",
    "promo_lift = ((promo_impact.loc['With Promo', 'mean'] / promo_impact.loc['No Promo', 'mean']) - 1) * 100\n",
    "print(f\"Avg sales lift from promotions: {promo_lift:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86e2c19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  mean  median          std   count\n",
      "StoreType                                          \n",
      "a          6855.150511  6283.0  3054.902223  439569\n",
      "b          9654.975867  8913.0  4271.941298   14420\n",
      "c          6916.547880  6421.0  2797.712897  109055\n",
      "d          6831.921627  6419.0  2494.009973  249780\n",
      "            mean  median\n",
      "Mon  8089.966816  7531.0\n",
      "Tue  7042.201993  6509.0\n",
      "Wed  6711.440669  6224.0\n",
      "Thu  6745.887804  6257.0\n",
      "Fri  7039.527111  6589.0\n",
      "Sat  5851.409575  5433.0\n",
      "Sun  7319.967809  6609.0\n"
     ]
    }
   ],
   "source": [
    "if 'StoreType' in df_feat.columns:\n",
    "    print(df_feat.groupby('StoreType')['Sales'].agg(['mean', 'median', 'std', 'count']))\n",
    "dow_stats = df_feat.groupby('DayOfWeek')['Sales'].agg(['mean', 'median'])\n",
    "dow_stats.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "print(dow_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0bd41e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chart saved: Top 20 Features - Correlation Heatmap.html\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "top_features = correlation_matrix['Sales'].abs().sort_values(ascending=False).head(20).index\n",
    "corr_subset = correlation_matrix.loc[top_features, top_features]\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr_subset,\n",
    "    text_auto=\".2f\",\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    origin='lower',\n",
    "    title=\"Top 20 Features - Correlation Heatmap\"\n",
    ")\n",
    "\n",
    "save_plotly_chart(fig, 'Top 20 Features - Correlation Heatmap.html')  # âœ… Saves externally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb04e9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chart saved: Interactive ACF and PACF Plots.html\n"
     ]
    }
   ],
   "source": [
    "sample_sales = df_feat.groupby('Date')['Sales'].mean().dropna().sample(min(900, len(df_feat)))\n",
    "# Compute ACF and PACF values (up to 50 lags)\n",
    "lags = 50\n",
    "acf_values = acf(sample_sales, nlags=lags)\n",
    "pacf_values = pacf(sample_sales, nlags=lags)\n",
    "# Create a 1x2 subplot layout\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Autocorrelation Function (ACF)\", \"Partial Autocorrelation (PACF)\"))\n",
    "# ACF plot\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(range(lags + 1)), y=acf_values, name='ACF', marker_color='skyblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "# PACF plot\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(range(lags + 1)), y=pacf_values, name='PACF', marker_color='lightgreen'),\n",
    "    row=1, col=2\n",
    ")\n",
    "# Add horizontal zero lines\n",
    "for i in range(1, 3):\n",
    "    fig.add_shape(type=\"line\", x0=0, x1=lags, y0=0, y1=0, line=dict(color=\"black\", width=1), row=1, col=i)\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"ACF and PACF Plots (Interactive)\",\n",
    "    showlegend=False,\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "save_plotly_chart(fig, 'Interactive ACF and PACF Plots.html')  # âœ… Saves externally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c86c47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_feat.to_csv('data/cleaned_sales_features.csv', index=False)\n",
    "#print(\" Cleaned feature dataset saved to: data/cleaned_sales_features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8f19e",
   "metadata": {},
   "source": [
    "## Forecasting Model Development and Optimization Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "902ae6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "#how to fix pmdarima installation issues?\n",
    "#import pmdarima as pm\n",
    "# machine learning libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25ea6ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(812824, 48)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "749dab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sales = df_feat.groupby('Date').agg({\n",
    "    'Sales': 'sum',\n",
    "    'Customers': 'sum',\n",
    "    'Promo': 'mean',\n",
    "    'SchoolHoliday': 'max',\n",
    "    'StateHoliday': lambda x: (x != '0').any().astype(int)\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5680ac72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Daily aggregated sales: 912 days\n",
      "ðŸ“… Date Range: 2013-01-31 00:00:00 to 2015-07-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "daily_sales = daily_sales.sort_values('Date')\n",
    "print(f\"ðŸ“Š Daily aggregated sales: {len(daily_sales)} days\")\n",
    "print(f\"ðŸ“… Date Range: {daily_sales['Date'].min()} to {daily_sales['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af67e808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Training: 870 days (2013-01-31 00:00:00 to 2015-06-19 00:00:00)\n",
      "ðŸ“Š Testing: 42 days (2015-06-20 00:00:00 to 2015-07-31 00:00:00)\n"
     ]
    }
   ],
   "source": [
    "test_weeks = 6\n",
    "test_size = test_weeks * 7\n",
    "train_size = len(daily_sales) - test_size\n",
    "\n",
    "train_data = daily_sales.iloc[:train_size].copy()\n",
    "test_data = daily_sales.iloc[train_size:].copy()\n",
    "\n",
    "print(f\"ðŸ“ˆ Training: {len(train_data)} days ({train_data['Date'].min()} to {train_data['Date'].max()})\")\n",
    "print(f\"ðŸ“Š Testing: {len(test_data)} days ({test_data['Date'].min()} to {test_data['Date'].max()})\")\n",
    "y_true = test_data['Sales'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a370a0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chart saved: 01_train_test_split.html\n"
     ]
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=train_data['Date'], y=train_data['Sales'],\n",
    "                        mode='lines', name='Training Data', line=dict(color='blue')))\n",
    "fig.add_trace(go.Scatter(x=test_data['Date'], y=test_data['Sales'],\n",
    "                        mode='lines', name='Test Data', line=dict(color='red')))\n",
    "fig.update_layout(title='Train-Test Split - Daily Aggregated Sales', height=400)\n",
    "save_plotly_chart(fig, '01_train_test_split.html')\n",
    "results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "adbb47e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Metrics function ready!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "def calculate_metrics(y_true, y_pred, model_name=None):\n",
    "    \"\"\"Calculate comprehensive metrics with safe MAPE handling\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Safe MAPE (avoid division by zero)\n",
    "    y_true_safe = np.maximum(y_true, 1)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100\n",
    "    \n",
    "    # RÂ² score\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    result = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    if model_name:\n",
    "        print(f\"âœ“ {model_name:20s} â†’ RMSE: {rmse:>10,.0f} | MAE: {mae:>10,.0f} | MAPE: {mape:>7.2f}% | RÂ²: {r2:>7.4f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… Metrics function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e5fc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ BASELINE MODELS (Benchmarks)\n",
      "======================================================================\n",
      "âœ“ Naive (Last Value)   â†’ RMSE:  3,442,355 | MAE:  2,425,738 | MAPE:  516.39% | RÂ²: -0.3498\n",
      "âœ“ Seasonal Naive (7-day) â†’ RMSE:  1,651,649 | MAE:  1,094,949 | MAPE:   16.33% | RÂ²:  0.6893\n",
      "âœ“ Moving Average (7-day) â†’ RMSE:  3,088,281 | MAE:  2,148,412 | MAPE:  458.57% | RÂ²: -0.0864\n",
      "\n",
      "âœ“ Baseline RMSE range: 3,442,355 - 1,651,649\n",
      "âœ“ All other models must beat these baselines!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ¯ BASELINE MODELS (Benchmarks)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Naive forecast (last train value)\n",
    "naive_forecast = np.repeat(train_data['Sales'].iloc[-1], len(test_data))\n",
    "naive_metrics = calculate_metrics(y_true, naive_forecast, \"Naive (Last Value)\")\n",
    "results_list.append(('Naive', naive_metrics))\n",
    "\n",
    "# Seasonal Naive (7-day)\n",
    "seasonal_naive = np.tile(train_data['Sales'].iloc[-7:].values, \n",
    "                         (len(test_data) // 7) + 1)[:len(test_data)]\n",
    "seasonal_metrics = calculate_metrics(y_true, seasonal_naive, \"Seasonal Naive (7-day)\")\n",
    "results_list.append(('Seasonal Naive', seasonal_metrics))\n",
    "\n",
    "# Moving Average (7-day)\n",
    "ma_forecast = np.repeat(train_data['Sales'].tail(7).mean(), len(test_data))\n",
    "ma_metrics = calculate_metrics(y_true, ma_forecast, \"Moving Average (7-day)\")\n",
    "results_list.append(('Moving Average', ma_metrics))\n",
    "\n",
    "print(f\"\\nâœ“ Baseline RMSE range: {naive_metrics['RMSE']:,.0f} - {seasonal_metrics['RMSE']:,.0f}\")\n",
    "print(f\"âœ“ All other models must beat these baselines!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b8c9e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ”„ MODEL 1: SMART ARIMA (Grid Search)\n",
      "======================================================================\n",
      "Searching ARIMA parameters...\n",
      "âœ“ Best ARIMA: (2, 0, 3) (AIC: 28229)\n",
      "âœ“ ARIMA(2, 0, 3)       â†’ RMSE:  2,504,064 | MAE:  2,050,757 | MAPE:  249.83% | RÂ²:  0.2857\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ”„ MODEL 1: SMART ARIMA (Grid Search)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_aic = np.inf\n",
    "best_order = None\n",
    "best_arima_model = None\n",
    "\n",
    "print(\"Searching ARIMA parameters...\")\n",
    "\n",
    "for p in range(0, 4):\n",
    "    for d in range(0, 2):\n",
    "        for q in range(0, 4):\n",
    "            try:\n",
    "                model = ARIMA(train_data['Sales'].values, order=(p, d, q))\n",
    "                result = model.fit()\n",
    "                if result.aic < best_aic:\n",
    "                    best_aic = result.aic\n",
    "                    best_order = (p, d, q)\n",
    "                    best_arima_model = result\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "print(f\"âœ“ Best ARIMA: {best_order} (AIC: {best_aic:.0f})\")\n",
    "\n",
    "arima_forecast = best_arima_model.forecast(steps=len(test_data))\n",
    "arima_metrics = calculate_metrics(y_true, arima_forecast, f\"ARIMA{best_order}\")\n",
    "results_list.append((f'ARIMA{best_order}', arima_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6d110c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ”® MODEL 2: FACEBOOK PROPHET (With Regressors)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:02:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "01:02:32 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Prophet              â†’ RMSE:    615,994 | MAE:    470,823 | MAPE:   24.99% | RÂ²:  0.9568\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ”® MODEL 2: FACEBOOK PROPHET (With Regressors)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data for Prophet\n",
    "prophet_train = train_data[['Date', 'Sales', 'Promo', 'SchoolHoliday']].copy()\n",
    "prophet_train.columns = ['ds', 'y', 'promo', 'school_holiday']\n",
    "\n",
    "# Train Prophet\n",
    "model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.05,\n",
    "    seasonality_prior_scale=15.0,\n",
    "    seasonality_mode='multiplicative',\n",
    "    interval_width=0.95\n",
    ")\n",
    "model.add_regressor('promo')\n",
    "model.add_regressor('school_holiday')\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model.fit(prophet_train)\n",
    "\n",
    "# Forecast\n",
    "future = pd.DataFrame({\n",
    "    'ds': test_data['Date'],\n",
    "    'promo': test_data['Promo'].values,\n",
    "    'school_holiday': test_data['SchoolHoliday'].values\n",
    "})\n",
    "\n",
    "prophet_forecast = model.predict(future)['yhat'].values\n",
    "prophet_metrics = calculate_metrics(y_true, prophet_forecast, \"Prophet\")\n",
    "results_list.append(('Prophet', prophet_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d90ad8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ MODEL 3: XGBOOST (ML with Features)\n",
      "======================================================================\n",
      "Preparing features...\n",
      "Training XGBoost...\n",
      "âœ“ XGBoost              â†’ RMSE:     64,389 | MAE:     53,351 | MAPE:    1.64% | RÂ²:  0.9995\n",
      "\n",
      "ðŸ“Š Top 15 Features:\n",
      "                  Feature  Importance\n",
      "0               Customers    0.604569\n",
      "21     sales_per_customer    0.280619\n",
      "8              is_weekend    0.043440\n",
      "3            StateHoliday    0.022373\n",
      "1                   Promo    0.016485\n",
      "4             day_of_week    0.008767\n",
      "15           sales_lag_21    0.005330\n",
      "19  sales_rolling_mean_14    0.002846\n",
      "16       customers_lag_21    0.001826\n",
      "9             sales_lag_1    0.001729\n",
      "18    sales_rolling_std_7    0.001603\n",
      "2           SchoolHoliday    0.001524\n",
      "7                    week    0.001499\n",
      "14       customers_lag_14    0.001488\n",
      "20   sales_rolling_std_14    0.001223\n",
      "âœ… Chart saved: M3_03_xgboost_features.html\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ MODEL 3: XGBOOST (ML with Features)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def prepare_xgb_features(df, lags=[1, 7, 14, 21]):\n",
    "    \"\"\"Create temporal + lag + rolling features\"\"\"\n",
    "    df_model = df.copy()\n",
    "    \n",
    "    # Temporal\n",
    "    df_model['day_of_week'] = df_model['Date'].dt.dayofweek\n",
    "    df_model['month'] = df_model['Date'].dt.month\n",
    "    df_model['quarter'] = df_model['Date'].dt.quarter\n",
    "    df_model['week'] = df_model['Date'].dt.isocalendar().week\n",
    "    df_model['is_weekend'] = (df_model['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Lags\n",
    "    for lag in lags:\n",
    "        df_model[f'sales_lag_{lag}'] = df_model['Sales'].shift(lag)\n",
    "        df_model[f'customers_lag_{lag}'] = df_model['Customers'].shift(lag)\n",
    "    \n",
    "    # Rolling\n",
    "    for window in [7, 14]:\n",
    "        df_model[f'sales_rolling_mean_{window}'] = df_model['Sales'].rolling(window).mean()\n",
    "        df_model[f'sales_rolling_std_{window}'] = df_model['Sales'].rolling(window).std()\n",
    "    \n",
    "    # Business\n",
    "    df_model['sales_per_customer'] = df_model['Sales'] / (df_model['Customers'] + 1)\n",
    "    \n",
    "    return df_model.dropna()\n",
    "\n",
    "print(\"Preparing features...\")\n",
    "train_prepared = prepare_xgb_features(train_data)\n",
    "test_prepared = prepare_xgb_features(test_data)\n",
    "\n",
    "feature_cols = [col for col in train_prepared.columns if col not in ['Date', 'Sales']]\n",
    "X_train = train_prepared[feature_cols]\n",
    "y_train = train_prepared['Sales']\n",
    "X_test = test_prepared[feature_cols]\n",
    "y_test_xgb = test_prepared['Sales'].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train_scaled, y_train, verbose=False)\n",
    "\n",
    "xgb_forecast = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Align lengths (feature engineering drops rows)\n",
    "min_len = min(len(test_data), len(xgb_forecast))\n",
    "xgb_forecast = xgb_forecast[:min_len]\n",
    "y_true_xgb = test_data['Sales'].values[-min_len:]\n",
    "\n",
    "xgb_metrics = calculate_metrics(y_true_xgb, xgb_forecast, \"XGBoost\")\n",
    "results_list.append(('XGBoost', xgb_metrics))\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "print(\"\\nðŸ“Š Top 15 Features:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Save feature importance chart\n",
    "fig = go.Figure(go.Bar(y=importance_df['Feature'], x=importance_df['Importance'], \n",
    "                       orientation='h', marker_color='skyblue'))\n",
    "fig.update_layout(title='XGBoost - Top 15 Feature Importance', height=600)\n",
    "save_plotly_chart(fig, 'M3_03_xgboost_features.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82566f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸŒ² MODEL 4: RANDOM FOREST\n",
      "======================================================================\n",
      "Training Random Forest...\n",
      "âœ“ Random Forest        â†’ RMSE:    108,398 | MAE:     60,042 | MAPE:    1.33% | RÂ²:  0.9987\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŒ² MODEL 4: RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "rf_forecast = rf_model.predict(X_test_scaled)[:min_len]\n",
    "rf_metrics = calculate_metrics(y_true_xgb, rf_forecast, \"Random Forest\")\n",
    "results_list.append(('Random Forest', rf_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8750c279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ§  MODEL 5: LSTM NEURAL NETWORK (Corrected)\n",
      "======================================================================\n",
      "Creating LSTM sequences...\n",
      "X_train: (863, 7, 3), y_train: (863,)\n",
      "X_test: (42, 7, 3), y_test: 42\n",
      "\n",
      "Building LSTM model...\n",
      "Training LSTM...\n",
      "âœ“ LSTM                 â†’ RMSE:    765,947 | MAE:    529,306 | MAPE:   38.31% | RÂ²:  0.9332\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ§  MODEL 5: LSTM NEURAL NETWORK (Corrected)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_lstm_sequences(train_df, test_df, lookback=7):\n",
    "    \"\"\"Create proper LSTM sequences\"\"\"\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    feature_set = ['Sales', 'Customers', 'Promo']\n",
    "    \n",
    "    # Fit scalers on train\n",
    "    scaler_X.fit(train_df[feature_set])\n",
    "    scaler_y.fit(train_df[['Sales']])\n",
    "    \n",
    "    # Transform\n",
    "    X_train_scaled = scaler_X.transform(train_df[feature_set])\n",
    "    y_train_scaled = scaler_y.transform(train_df[['Sales']]).flatten()\n",
    "    \n",
    "    # Create sequences\n",
    "    X_tr, y_tr = [], []\n",
    "    for i in range(lookback, len(train_df)):\n",
    "        X_tr.append(X_train_scaled[i-lookback:i])\n",
    "        y_tr.append(y_train_scaled[i])\n",
    "    \n",
    "    X_tr = np.array(X_tr)\n",
    "    y_tr = np.array(y_tr)\n",
    "    \n",
    "    # Test sequences\n",
    "    X_test_scaled = scaler_X.transform(test_df[feature_set])\n",
    "    combined = np.vstack([X_train_scaled[-lookback:], X_test_scaled])\n",
    "    \n",
    "    X_te = []\n",
    "    for i in range(lookback, len(combined)):\n",
    "        X_te.append(combined[i-lookback:i])\n",
    "    \n",
    "    X_te = np.array(X_te)\n",
    "    y_test_actual = test_df['Sales'].values[:len(X_te)]\n",
    "    \n",
    "    return X_tr, y_tr, X_te, y_test_actual, scaler_y\n",
    "\n",
    "print(\"Creating LSTM sequences...\")\n",
    "X_tr, y_tr, X_te, y_te_lstm, scaler_lstm = create_lstm_sequences(\n",
    "    train_data, test_data, lookback=7\n",
    ")\n",
    "\n",
    "print(f\"X_train: {X_tr.shape}, y_train: {y_tr.shape}\")\n",
    "print(f\"X_test: {X_te.shape}, y_test: {len(y_te_lstm)}\")\n",
    "\n",
    "# Build LSTM\n",
    "print(\"\\nBuilding LSTM model...\")\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(7, 3)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(25),\n",
    "    Dropout(0.2),\n",
    "    Dense(12, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "lstm_model.compile(optimizer=Adam(0.001), loss='mse')\n",
    "\n",
    "print(\"Training LSTM...\")\n",
    "lstm_model.fit(X_tr, y_tr, epochs=30, batch_size=64, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Predict\n",
    "y_pred_scaled = lstm_model.predict(X_te, verbose=0).flatten()\n",
    "y_lstm = scaler_lstm.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "lstm_metrics = calculate_metrics(y_te_lstm, y_lstm, \"LSTM\")\n",
    "results_list.append(('LSTM', lstm_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46a87941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸŽ­ ENSEMBLE MODEL (Weighted Voting)\n",
      "======================================================================\n",
      "âœ“ Ensemble             â†’ RMSE:    307,100 | MAE:    261,871 | MAPE:   19.88% | RÂ²:  0.9897\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ­ ENSEMBLE MODEL (Weighted Voting)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Align all forecasts to same length\n",
    "min_ensemble_len = min(len(y_true_xgb), len(y_te_lstm))\n",
    "\n",
    "ensemble_forecast = (\n",
    "    0.40 * xgb_forecast[:min_ensemble_len] +\n",
    "    0.30 * rf_forecast[:min_ensemble_len] +\n",
    "    0.20 * prophet_forecast[-min_ensemble_len:] +\n",
    "    0.10 * arima_forecast[-min_ensemble_len:]\n",
    ")\n",
    "\n",
    "y_true_ensemble = y_true_xgb[:min_ensemble_len]\n",
    "ensemble_metrics = calculate_metrics(y_true_ensemble, ensemble_forecast, \"Ensemble\")\n",
    "results_list.append(('Ensemble', ensemble_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d960c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ† FINAL MODEL COMPARISON\n",
      "======================================================================\n",
      "\n",
      "         Model      RMSE       MAE   MAPE      R2\n",
      "       XGBoost   64389.0   53351.0   1.64  0.9995\n",
      " Random Forest  108398.0   60042.0   1.33  0.9987\n",
      "      Ensemble  307100.0  261871.0  19.88  0.9897\n",
      "       Prophet  615994.0  470823.0  24.99  0.9568\n",
      "          LSTM  765947.0  529306.0  38.31  0.9332\n",
      "Seasonal Naive 1651649.0 1094949.0  16.33  0.6893\n",
      "ARIMA(2, 0, 3) 2504064.0 2050757.0 249.83  0.2857\n",
      "Moving Average 3088281.0 2148412.0 458.57 -0.0864\n",
      "         Naive 3442355.0 2425738.0 516.39 -0.3498\n",
      "\n",
      "======================================================================\n",
      "ðŸ¥‡ BEST MODEL: XGBoost\n",
      "   RMSE: 64,389\n",
      "   MAPE: 1.64%\n",
      "======================================================================\n",
      "âœ… Chart saved: M3_10_model_comparison.html\n",
      "\n",
      "âœ… Results saved: data/milestone3_model_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ† FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create results DataFrame with rounding\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': name,\n",
    "        'RMSE': round(metrics.get('RMSE', np.nan), 0),\n",
    "        'MAE': round(metrics.get('MAE', np.nan), 0),\n",
    "        'MAPE': round(metrics.get('MAPE', np.nan), 2),\n",
    "        'R2': round(metrics.get('R2', np.nan), 4)\n",
    "    }\n",
    "    for name, metrics in results_list\n",
    "])\n",
    "\n",
    "# Sort by RMSE ascending\n",
    "results_df = results_df.sort_values('RMSE', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Print table\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Best model (lowest RMSE)\n",
    "best_model_row = results_df.loc[results_df['RMSE'].idxmin()]\n",
    "best_model_name = best_model_row['Model']\n",
    "best_rmse = best_model_row['RMSE']\n",
    "best_mape = best_model_row['MAPE']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸ¥‡ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   RMSE: {best_rmse:,.0f}\")\n",
    "print(f\"   MAPE: {best_mape:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=['RMSE Comparison', 'MAE Comparison', 'MAPE Comparison', 'Model Ranking']\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['RMSE']), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['MAE']), row=1, col=2)\n",
    "fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['MAPE']), row=2, col=1)\n",
    "fig.add_trace(go.Bar(\n",
    "    x=results_df['Model'],\n",
    "    y=list(range(1, len(results_df) + 1))\n",
    "), row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"ðŸ“Š Model Performance Comparison\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "save_plotly_chart(fig, 'M3_10_model_comparison.html')\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('data/milestone3_model_results.csv', index=False)\n",
    "print(\"\\nâœ… Results saved: data/milestone3_model_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ec7ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“Š RESIDUAL ANALYSIS (Best Model)\n",
      "======================================================================\n",
      "Mean residual: 10611.38 (should be ~0)\n",
      "Std residual: 63508.17\n",
      "Min residual: -145,049\n",
      "Max residual: 121,447\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š RESIDUAL ANALYSIS (Best Model)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get best model's predictions\n",
    "if best_model_name == 'XGBoost':\n",
    "    best_predictions = xgb_forecast[:min_ensemble_len]\n",
    "    y_true_best = y_true_xgb[:min_ensemble_len]\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_predictions = rf_forecast[:min_ensemble_len]\n",
    "    y_true_best = y_true_xgb[:min_ensemble_len]\n",
    "elif best_model_name == 'Ensemble':\n",
    "    best_predictions = ensemble_forecast\n",
    "    y_true_best = y_true_ensemble\n",
    "else:\n",
    "    best_predictions = arima_forecast[-min_ensemble_len:]\n",
    "    y_true_best = y_true_ensemble\n",
    "\n",
    "residuals = y_true_best - best_predictions\n",
    "\n",
    "print(f\"Mean residual: {np.mean(residuals):.2f} (should be ~0)\")\n",
    "print(f\"Std residual: {np.std(residuals):.2f}\")\n",
    "print(f\"Min residual: {np.min(residuals):,.0f}\")\n",
    "print(f\"Max residual: {np.max(residuals):,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1bb6576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chart saved: M3_11_residual_analysis.html\n"
     ]
    }
   ],
   "source": [
    "fig = make_subplots(rows=2, cols=2,\n",
    "    subplot_titles=['Residuals Over Time', 'Distribution', 'Predicted vs Actual', 'Residuals vs Predicted'])\n",
    "\n",
    "# Time series\n",
    "fig.add_trace(go.Scatter(y=residuals, mode='markers+lines', name='Residuals'), row=1, col=1)\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=1, col=1)\n",
    "\n",
    "# Histogram\n",
    "fig.add_trace(go.Histogram(x=residuals, nbinsx=30, name='Distribution'), row=1, col=2)\n",
    "\n",
    "# Predicted vs Actual\n",
    "fig.add_trace(go.Scatter(y=y_true_best, mode='lines', name='Actual', line=dict(color='green')), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(y=best_predictions, mode='lines', name='Predicted', line=dict(color='red')), row=2, col=1)\n",
    "\n",
    "# Residuals vs Predicted\n",
    "fig.add_trace(go.Scatter(x=best_predictions, y=residuals, mode='markers', name='Res vs Pred'), row=2, col=2)\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=800, title_text=f\"Residual Analysis - {best_model_name}\", showlegend=False)\n",
    "save_plotly_chart(fig, 'M3_11_residual_analysis.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ba0bab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“ˆ FINAL FORECAST VISUALIZATION\n",
      "======================================================================\n",
      "âœ… Chart saved: M3_12_final_forecast.html\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“ˆ FINAL FORECAST VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_data['Date'], y=train_data['Sales'],\n",
    "    mode='lines', name='Training Data', line=dict(color='blue', width=1)))\n",
    "\n",
    "# Test data actual\n",
    "fig.add_trace(go.Scatter(x=test_data['Date'][-len(y_true_best):], y=y_true_best,\n",
    "    mode='lines', name='Actual Test Data', line=dict(color='green', width=2)))\n",
    "\n",
    "# Best forecast\n",
    "test_dates = test_data['Date'].values[-len(best_predictions):]\n",
    "fig.add_trace(go.Scatter(x=test_dates, y=best_predictions,\n",
    "    mode='lines', name=f'{best_model_name} Forecast', \n",
    "    line=dict(color='red', width=2, dash='dash')))\n",
    "\n",
    "# Confidence interval\n",
    "std_residual = np.std(residuals)\n",
    "fig.add_trace(go.Scatter(x=test_dates, y=best_predictions + std_residual,\n",
    "    fill=None, mode='lines', line_color='rgba(0,0,0,0)', showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=test_dates, y=best_predictions - std_residual,\n",
    "    fill='tonexty', mode='lines', line_color='rgba(0,0,0,0)',\n",
    "    name='Â±1 Std Dev', fillcolor='rgba(255,0,0,0.2)'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'ðŸ† Final Forecast: {best_model_name} (MAPE: {best_mape:.2f}%)',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Sales',\n",
    "    height=600,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "save_plotly_chart(fig, 'M3_12_final_forecast.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95a0ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ’¾ SAVING MODELS & RESULTS\n",
      "======================================================================\n",
      "âœ… XGBoost model saved\n",
      "âœ… Predictions exported: data/final_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ’¾ SAVING MODELS & RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save best model\n",
    "if 'xgb_model' in globals() and best_model_name == 'XGBoost':\n",
    "    joblib.dump(xgb_model, 'models/best_model.pkl')\n",
    "    print(\"âœ… XGBoost model saved\")\n",
    "elif 'rf_model' in globals() and best_model_name == 'Random Forest':\n",
    "    joblib.dump(rf_model, 'models/best_model.pkl')\n",
    "    print(\"âœ… Random Forest model saved\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_export = pd.DataFrame({\n",
    "    'Date': test_data['Date'].values[-len(y_true_best):],\n",
    "    'Actual': y_true_best,\n",
    "    'Predicted': best_predictions,\n",
    "    'Residual': residuals,\n",
    "    'Error_Percent': (np.abs(residuals) / y_true_best * 100)\n",
    "})\n",
    "predictions_export.to_csv('data/final_predictions.csv', index=False)\n",
    "print(\"âœ… Predictions exported: data/final_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d644f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Summary saved: data/milestone3_summary.json\n",
      "\n",
      "ðŸŽ‰ MILESTONE 3 COMPLETE!\n",
      "   Best Model: XGBoost\n",
      "   RMSE: 64,389\n",
      "   MAPE: 1.64%\n"
     ]
    }
   ],
   "source": [
    "summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'rmse': float(best_rmse),\n",
    "    'mae': float(results_df.iloc[best_idx]['MAE']),\n",
    "    'mape': float(best_mape),\n",
    "    'r2': float(results_df.iloc[best_idx]['R2']),\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'all_results': results_df.to_dict()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('data/milestone3_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "    \n",
    "print(\"âœ… Summary saved: data/milestone3_summary.json\")\n",
    "print(f\"\\nðŸŽ‰ MILESTONE 3 COMPLETE!\")\n",
    "print(f\"   Best Model: {best_model_name}\")\n",
    "print(f\"   RMSE: {best_rmse:,.0f}\")\n",
    "print(f\"   MAPE: {best_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c118c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
